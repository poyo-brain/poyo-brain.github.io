<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="keywords" content="">
    <meta name="author" content="Mehdi Azabou, Vinam Arora">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>POYO-1</title>
    <link rel="shortcut icon" href="assets/üß†.ico" />
    <script src="https://kit.fontawesome.com/3875b07657.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="https://bulma.io/css/bulma-docs.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
    <link rel="stylesheet" href="build/style.css">

    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-3.2.2.min.js"></script>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.2.min.js"></script>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.2.min.js"></script>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.2.min.js"></script>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.2.2.min.js"></script>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-api-3.2.2.min.js"></script>
    <script src="./js/mat4js.read.min.js"></script>
    <script type="module" src="./js/script.js"></script>
    <script src="./js/spike-vis.js"></script>
    <script src="./js/query-vis.js"></script>
    <script src="./js/sample-vis.js"></script>
    <script src="./js/finetune-vis.js"></script>

    <!-- <script type="module" src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r153/three.module.js"></script> -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-L6PHX4M5RR"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-L6PHX4M5RR');
    </script>
    <script>
        function resizeIframe(obj) {
            obj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';
        }
    </script>
</head>

<body>
    <section class="section title-section">
        <div>
            <h1 class="page-title">
                POYO-1
            </h1>
            <h2 class="title page-title">
                A Unified, Scalable Framework for Neural Population Decoding
            </h2>
        </div>
        <div class="header authors">
            <a href="https://www.mehai.dev">Mehdi Azabou</a><sup>1</sup>
            <a href="https://www.linkedin.com/in/vinam-arora/">Vinam Arora</a><sup>1</sup>
            <a href="https://venkys.website/">Venkataramana Ganesh</a><sup>1</sup>
            <a href="https://mila.quebec/en/person/ximeng-mao/">Ximeng Mao</a><sup>2,3</sup>
            <a href="https://www.linkedin.com/in/santoshnachimuthu/">Santosh Nachimuthu</a><sup>1</sup>
            <a href="https://www.linkedin.com/in/mmend/">Michael Mendelson</a><sup>1</sup>
            <a href="https://linclab.mila.quebec/team/blake">Blake Richards</a><sup>2,4,5</sup>
            <a href="https://ma.ttperi.ch/">Matthew Perich</a><sup>2,3</sup>
            <a href="https://www.guillaumelajoie.com/">Guillaume Lajoie</a><sup>2,3,5</sup>
            <a href="https://dyerlab.gatech.edu/">Eva L. Dyer</a><sup>1</sup>
        </div>
        <div class="affiliations">
            <sup>1</sup> <img src="./assets/georgia_tech.png" alt="Georgia Tech logo" class="affiliation-logo"> Georgia Institute of Technology,
            <sup>2</sup> <img src="./assets/mila.png" alt="MILA logo" class="affiliation-logo"> Mila,
            <sup>3</sup> <img src="./assets/uni_montreal.png" alt="Universit√© de Montr√©al logo" class="affiliation-logo"> Universit√© de Montr√©al,
        </div>
        <div class="affiliations">
            <sup>4</sup> <img src="./assets/mcgill_university.png" alt="McGill University logo" class="affiliation-logo"> McGill University,
            <sup>5</sup> <img src="./assets/cifar.png" alt="CIFAR logo" class="affiliation-logo"> CIFAR
        </div>
        <div class="links">
            <a href="">
                <i class="fa fa-file"></i> Paper
            </a>
            <a href="https://github.com/nerdslab/poyo">
                <i class="fa fa-github"></i>
                Code
            </a>
        </div>


    </section>

    <aside class="sidebar">
        <ul>
            <li> Contents </li>
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#datasets">Datasets & Challenges</a></li>
            <!-- <li><a href="#heterogeneity">Data Heterogeneity</a></li> -->
            <!-- <li><a href="#tokenization">Tokenization</a></li> -->
            <li><a href="#tokenization">Tokenization</a></li>
            <li><a href="#architecture">Architecture</a></li>
            <li><a href="#pretraining">Pretraining</a></li>
            <li><a href="#finetuning">Finetuning</a></li>
            <!-- Add more sections as needed -->
        </ul>
    </aside>


    <div class="main-content">

        <section class="section" id="abstract">
            <h3 class="section-title">
                Abstract
            </h3>
            <p>
                Our ability to use deep learning approaches to decipher neural activity would likely benefit from
                greater scale, in terms of both the model size and the datasets. However, the integration of many neural
                recordings into one unified model is challenging, as each recording contains the activity of different
                neurons from different individual animals.
            </p>

            <p>
                In this paper, we introduce a training framework and
                architecture designed to model the population dynamics of neural activity across diverse, large-scale
                neural recordings. Our method first tokenizes individual spikes within the dataset to build an efficient
                representation of neural events that captures the fine temporal structure of neural activity. We then
                employ cross-attention and a PerceiverIO backbone to further construct a latent tokenization of neural
                population activities. Utilizing this architecture and training framework, we construct a large- scale
                multi-session model trained on large datasets from seven nonhuman primates, spanning over 158 different
                sessions of recording from over 27,373 neural units and over 100 hours of recordings.
            </p>

            <p>
                In a number of
                different tasks, we demonstrate that our pretrained model can be rapidly adapted to new, unseen sessions
                with unspecified neuron correspondence, enabling few-shot performance with minimal labels. This work
                presents a powerful new approach for building deep learning tools to analyze neural data and stakes out
                a clear path to training at scale for neural decoding models.
            </p>

            <!-- <p class="question">
                &#8594 Can we [ ] ?
            </p>

            <p>
                We introduce POYO-1 [...].
            </p> -->
            <!-- 
            <div class="table-of-contents">
                <h2>Table of Contents</h2>
                <ul>
                    <li><a href="#section1"> Section 1</a>: Description.</li>
                    <li><a href="#section2"> Section 2</a>: Description.</li>
                    <li><a href="#section3">Section 3</a>: Description.</li>
                </ul>
            </div> -->
        </section>

        <section class="section" id="datasets">
            <h3 class="section-title">
                <a href="datasets"> </a>Datasets & Challenges
            </h3>
            <!-- <h3 class="subsection-title">
                <i class="fa-regular fa-circle fa-2xs"></i> Spiking neural activity
            </h3> -->
            <!-- <p>
                One of the key advantages of our approach is its ability to scale to handle large amounts of neural
                data, including sessions from different numbers of neurons, across different tasks and recording setups,
                and from different animals. Thus we set out to build a diverse dataset large enough to test our
                approach.
            </p> -->
            <p>
                We curated a multi-lab dataset with electrophysiological recordings from motor cortical
                regions. In total, we aggregated 178 sessions worth of data,
                with 29,453 units from the primary motor (M1), premotor (PMd), and primary somatosensory (S1)
                regions in the cortex of 9 nonhuman primates.
            </p>

            <div class="table">
                <table border="1">
                    <thead>
                        <tr>
                            <th>Study</th>
                            <th>Regions</th>
                            <th>Tasks</th>
                            <th># Individuals</th>
                            <th># Sessions</th>
                            <th># Units</th>
                            <th># Spikes</th>
                            <th># Behavior Timepoints </th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Perich et al.</td>
                            <td>M1, PMd</td>
                            <td>Center-Out, Random Target</td>
                            <td>4</td>
                            <td>117</td>
                            <td>11,557</td>
                            <td>143M</td>
                            <td>20M</td>
                        </tr>
                        <tr>
                            <td>Churchland et al.</td>
                            <td>M1</td>
                            <td>Center-Out</td>
                            <td>2</td>
                            <td>9</td>
                            <td>1,728</td>
                            <td>706M</td>
                            <td>87M</td>
                        </tr>
                        <tr>
                            <td>Makin et al.</td>
                            <td>M1, S1</td>
                            <td>Random Target</td>
                            <td>2</td>
                            <td>47</td>
                            <td>14,899</td>
                            <td>123M</td>
                            <td>15M</td>
                        </tr>
                        <tr>
                            <td>Flint et al.</td>
                            <td>M1</td>
                            <td>Center-Out</td>
                            <td>1</td>
                            <td>5</td>
                            <td>957</td>
                            <td>7.9M</td>
                            <td>0.3M</td>
                        </tr>
                        <tr>
                            <td>NLB-Maze</td>
                            <td>M1</td>
                            <td>Maze</td>
                            <td>1</td>
                            <td>1</td>
                            <td>182</td>
                            <td>3.6M</td>
                            <td>6.8M</td>
                        </tr>
                        <tr>
                            <td>NLB-RTT</td>
                            <td>M1, S1</td>
                            <td>Random Target</td>
                            <td>1</td>
                            <td>1</td>
                            <td>130</td>
                            <td>1.5M</td>
                            <td>2.8M</td>
                        </tr>
                    </tbody>
                </table>
                <div class="caption">
                    <p> <b>Table 1:</b> Datasets used in this work </p>
                </div>
            </div>

            <!-- <p>We place this in the context of standard
                analyses within a single lab or paper which typically involve 10‚Äôs of sessions and a few hundred
                neurons.
            </p> -->

            <p>
                All of these neural recordings were collected while the animals performed various motor tasks that vary
                in their inherent complexity:
            </p>

            <div class="task-expl-container">
                <!-- Row 1 -->
                <div class="task-expl-one-container">
                    <div class="task-expl-diagram-container">
                        <img src="./assets/tasks-04.png" alt="Description for Image 1">
                        <canvas id="webgl1"></canvas>
                    </div>
                    <div class="task-expl-text">
                        <b>Center-Out task (CO)</b>: This task is relatively stereotyped, with
                        the animal making a reach to one of eight fixed targets after receiving a go cue, and then
                        returning to the
                        center.
                    </div>
                </div>

                <!-- Row 2 -->
                <div class="task-expl-one-container">
                    <div class="task-expl-diagram-container">
                        <img src="./assets/tasks-05.png" alt="Description for Image 2">
                        <canvas id="webgl2"></canvas>
                    </div>
                    <div class="task-expl-text">
                        <b>Random Target task (RT)</b>: The animal makes continuous and
                        self-paced
                        movements with new targets appearing in succession at random locations on the screen.
                    </div>
                </div>

                <!-- Row 3 -->
                <div class="task-expl-one-container">
                    <div class="task-expl-diagram-container">
                        <img src="./assets/tasks-06.png" alt="Description for Image 3">
                        <canvas id="webgl3"></canvas>
                    </div>
                    <div class="task-expl-text">
                        <b>Touchscreen Random Target task (RT)</b>: The animal makes reaches to targets arranged in an
                        8x8 grid without pre-movement delays.
                    </div>
                </div>

                <!-- Row 4 -->
                <div class="task-expl-one-container">
                    <div class="task-expl-diagram-container">
                        <img src="./assets/tasks-07.png" alt="Description for Image 4">
                        <canvas id="webgl4"></canvas>
                    </div>
                    <div class="task-expl-text">
                        <div class="red-tag">
                            Held-out for Testing
                        </div>
                        <p>
                            <b>Maze task</b>: In this task,
                            a monkey performing reaches with instructed delays to visually presented targets while
                            avoiding
                            the
                            boundaries of a virtual maze.
                        </p>
                    </div>
                </div>

            </div>

            <h3 class="subsection-title" style="margin-bottom: 0.5rem;">
                Challenges
            </h3>
            <section class="subsection">
                <!-- <h3 class="subsubsection-title">
                    Variability of neural recordings across days and individuals
                </h3> -->
                <p>
                    <i class="fa-solid fa-arrow-right"></i> <b class="subsubsection-title">Variability of neural
                        recordings across days and individuals</b>: Outside of knowing which region of the brain was
                    implanted, the identity of the neurons that are
                    recorded is unknown: we are dealing with unlabelled channels. This means that we don't know which
                    neurons are
                    being recorded from, or how many we are able to record.
                    <!-- <p>
                    Even for the same individual, we don't know whether we're recording from the same neurons across
                    multiple
                    days. An electrode that was capturing the activity of a particular neuron one day might be slightly
                    off-target the next day. This is due to factors like electrode drift. -->
                    <!-- <li> Physical Movement: The brain is a soft tissue, and even minor movements (e.g., due to breathing,
                minor head adjustments, or tissue swelling) can shift the relative position of neurons with respect to
                the electrode. This means that an electrode that was capturing the activity of a particular neuron one
                day might be slightly off-target the next day.</li>

                <li> Electrode Drift: Over time, the electrode itself can shift within the tissue. This can be due to
                various reasons, including the natural healing processes of the tissue around the implanted electrode,
                or mechanical factors related to the electrode's design.</li>

                <li> Tissue Response: The insertion and presence of an electrode can cause a tissue response. This
                might include inflammation, gliosis (formation of a scar-like tissue around the electrode), or neuronal
                death around the insertion site. Over time, these changes can alter the local neural environment,
                affecting which neurons are in proximity to the electrode.</li>

                <li> Neuronal Variability: Even if you were recording from the same neuron, its activity might
                naturally vary from day to day due to changes in the animal's state, experiences, or other factors. This
                variability can sometimes make it seem as though you're recording from different neurons, even if you
                aren't.</li>

                <li> Signal Quality: The quality of recordings can change over time. Factors like electrode
                degradation, changes in impedance, or accumulation of biological material on the electrode surface can
                alter the signal-to-noise ratio, potentially masking spikes from certain neurons.</li>

                Because of these challenges, while it's possible to record from the same general region of the brain
                across multiple days, it's difficult to guarantee that you're recording from the exact same neurons.
                Researchers often use statistical methods and other techniques to assess the stability of their
                recordings and to determine whether they are likely recording from the same or different neuronal
                populations across sessions.</li> -->
                    <!-- 
            <div class="blocks-container">
                <div class="block">
                    <i class="fa-solid fa-triangle-exclamation" style="color: #f6de2f;"></i>
                    <p><b>Physical Movement</b>: The brain is a soft tissue that moves e.g., due to breathing,
                        minor head adjustments, or tissue swelling. </p>
                </div>

                <div class="block">
                    <i class="fa-solid fa-triangle-exclamation" style="color: #f6de2f;"></i>
                    <p><b>Electrode Drift</b>: Over time, the electrode itself can shift within the tissue, typically
                        due to the natural healing of the tissue around the implanted electrode.</p>
                </div>
                <div class="block">
                    <i class="fa-solid fa-triangle-exclamation" style="color: #f6de2f;"></i>
                    <p><b>Tissue Response</b>: The insertion and presence of an electrode can cause a tissue response.
                        This might include inflammation, gliosis, or neuronal death around the insertion site.</p>
                </div>
                <div class="block">
                    <i class="fa-solid fa-triangle-exclamation" style="color: #f6de2f;"></i>
                    <p><b>Neuronal Variability</b>: Even if you were recording from the same neuron, its activity might
                        naturally vary from day to day due to changes in the animal's state, experiences, or other
                        factors.</p>
                </div>
                <div class="block">
                    <i class="fa-solid fa-triangle-exclamation" style="color: #f6de2f;"></i>
                    <p><b>Signal Quality</b>: Electrode degradation, changes in impedance, or accumulation of biological
                        material on the electrode
                        surface can alter the signal-to-noise ratio.</p>
                </div>
            </div> -->

                    <!-- <p>
                The spiking activity of a population of neurons is recorded by placing electrodes near the neurons,
                and measuring the voltage changes that occur when a neuron fires. 
            </p> -->
                    <!-- 
            <div class="figure">
                <img src="assets/electrode_shift.png" alt="Electrode shift">
                <div class="caption">
                    <p>  Over time, the electrode can shift within the tissue, typically
                        due to the natural healing of the tissue around the implanted electrode 
                        and minor movements (e.g., due to breathing,
                        minor head adjustments).</p>
                </div>
            </div> -->



                    <!-- </section>

        <section class="section" id="heterogeneity">
            <h3 class="section-title">
                <a href=""> </a>Heterogeneity across Datasets
            </h3> -->
                </p>
                <p>
                    <i class="fa-solid fa-arrow-right"></i> <b class="subsubsection-title">Variablity in data
                        processing</b>: Different
                    algorithms are used to process the electrophysiological recordings. Some datasets are spike-sorted,
                    others are threshold-crossed. The quality and type of units will thus vary.
                </p>

                <p>
                    <i class="fa-solid fa-arrow-right"></i> <b class="subsubsection-title">Heterogeneity of behavior
                        across datasets</b>:
                    The datasets were collected from multiple labs, using different equipment, and with different
                    experimental protocols. The studied motor tasks vary in their inherent complexity.
                </p>
                <!-- <canvas class="webgl"></canvas> -->

                <!-- <p>
                <i class="fa-solid fa-arrow-right"></i> <b>Variability of controllers</b>: While some animals perform
                the tasks using a manipulandum, others
                use a touch screen.
            </p> -->

                <p>
                    <i class="fa-solid fa-arrow-right"></i> <b class="subsubsection-title">Variablity in sampling
                        rate</b>: The sampling rate at
                    which
                    these behaviors were recorded varies
                    from lab to lab (100Hz to 1kHz).
                </p>

                <!-- <section class="section" id="tokenization">
            <h3 class="section-title">
                <a href=""> </a>Spike Tokenization
            </h3>
            <h3 class="subsection-title">
                Previous approaches: Binning
            </h3>

            <p>
                Because of the irregular nature of neural spiking activity, established approaches usually rely on
                temporally
                binned analyses: time is divided into discrete intervals, and the number of spikes for a unit within
                each interval is counted. Binned representations regularize the structure of spiking data and enable
                compatibility with existing machine learning frameworks.
            </p>

            <iframe src="./spike_binning.html" width="900" height="400" scrolling="no"
                style="overflow:hidden;"></iframe>

            <p>
                However, the choice of bin size presents a trade-off: longer windows (> 100ms) simplify the temporal
                dimension but sacrifice crucial spike-timing information, while shorter
                windows (1 ms) maintain individual spikes but result in an extremely sparse data format.
            </p>

            <h3 class="subsection-title">
                Our solution
            </h3>

            <p>
                By avoiding the need to aggregate or bin the neural activity, our approach can be more
                computationally efficient than previous methods while still maintaining high decoding accuracy. Overall,
                our choice of tokenizing spikes directly offers a promising new approach to neural decoding that
                combines high temporal resolution with computational efficiency.
            </p>

            <p>
                The neural tokenizer. Based upon these motivations, we propose a novel approach for tokenizing neural
                population activity where each spike is represented as a token. Each token contains information about
                the timing of a spike and the unique identity of its unit (the neuron it belongs to). As such, we
                represent our input as a set of tokens of arbitrary length. This is different from the established
                approach of temporal binning (counting the number of spikes in fixed temporal windows), which is
                typically used with neural data. Because we represent the input data as a set of arbitrary size we never
                define or fix the expected number of units, so the model can ingest arbitrary populations of units, and
                thus can be trained across many datasets. At the same time, by representing the time of each spike and
                avoiding representing the times where the neuron doesn‚Äôt spike, we avoid the vectors containing mostly
                zeros, as is common in binned representations of neural activity.
            </p>

            <p>
                To make this precise, we assign each unit a unique identifier and a corresponding D-dimensional
                learnable embedding. Let Embed(¬∑) denote a lookup table that associates each unit to its unit embedding.
                Akin to word embeddings that encode semantic meaning and relationship between words, the unit embedding
                space encodes something about the ‚Äúspeaker‚Äôs‚Äù identity and role in neural computation. A unit fires a
                sequence of spikes in a window of time [0, T ]. Each spike i will be represented by a token
                characterized by (xi, ti), where xi = Embed(spike i‚Ä≤s unit) is the token‚Äôs embedding and ti is the spike
                timestamp. Collectively, and for an arbitrary set of units, we combine all the spikes into a sequence of
                length M . The population neural activity is represented by [x1 , . . . , xM ] and their corresponding
                times [t1, . . . , tM ]. While T represents the size of our context window, M will vary depending on the
                number of units in the population and their firing rates (see Figure 1). Note that all spikes from a
                specific unit have the same embedding xi, and only differ in their timing.
            </p>

        </section> -->

            </section>
        </section>


        <section class="section" id="tokenization">
            <h3 class="section-title">
                <a href=""> </a> Tokenization
            </h3>

            <h3 class="subsection-title">
                Spike Tokenization
            </h3>
            <p>
                We propose a novel approach for tokenizing neural population activity where each spike is represented as
                a token. Each token contains information about the timing of a spike and the unique identity of its unit
                (the neuron it belongs to). As such, we represent our input as a set of tokens of arbitrary length.
            </p>

            <div style="color: #808080;">
                <img src="assets/bk-tap.png" style="width: 30px;">
                Try adding new spikes to one of the three units, by tapping on the <b>Spikes</b> plot below, and see how
                new tokens are added.
            </div>


            <h3 class="subsection-title">
                Behavior Query Tokens
            </h3>
            <p>
                The length of the hand velocity sequence will depend on the sampling frequency, which differs
                significantly across datasets. We propose a flexible mechanism for predicting outputs of varying
                lengths by querying our model one point at a time.
            </p>
            <p>
                To account for variability in experimental conditions, we add a session-level embedding to every ouput
                query.
                The output query token is thus defined by its timestamp, and its corresponding learnable session
                embedding.
            </p>

            <!-- Align elements to the center-->
            <div style="color: #808080;">
                <img src="assets/bk-tap.png" style="width: 30px;">
                Try querying the model at new timestamps, by tapping on the <b>Query Timestamps</b> plot below, and see
                how the model responds.
            </div>

            <div id="token-vis-container">
                <div id="token-vis-spike-container">
                    <div id="token-vis-spikes"></div>
                    <img src="assets/spike-connector.svg" />
                    <div id="token-vis-spike-embs"></div>
                </div>
                <div id="token-vis-middle">
                    <div id="query-vis-1"></div>
                    <img id="token-vis-model-img" src="assets/tokenization-blackbox.svg" alt="model-blackbox">
                </div>
                <div id="token-vis-vel-plots">
                    <div id="token-vis-vx"></div>
                    <div id="token-vis-vy"></div>
                </div>
                <div class="square" id="animatedSquare"></div>

            </div>

        </section>


        <section class="section" id="architecture">
            <h3 class="section-title">
                <a href=""> </a> Architecture
            </h3>

            <p>
                With our tokenization techniques, the problem of Neural Decoding
                boils down to learning a model that converts a sequence of spike
                tokens to a sequence of hand-velocities.
            </p>
            <p>
                Our approach utilizes the Transformer architecture, recognized for
                its excellence in handling sequence-to-sequence transformations.
                Given the difference in number of tokens between input and output and
                the potentially large number of spikes, we've adopted the PerceiverIO
                architecture <a href="https://www.deepmind.com/open-source/perceiver-io">
                    [Jaegle et al.]</a> instead of the standard transformer
                <a href="https://arxiv.org/abs/1706.03762">[Vaswani et al.]</a>.
                To inject token timestamps into the attention
                blocks, we use Rotary Position Embeddings
                <a href="https://arxiv.org/abs/2104.09864">[Su et. al.]</a>,
                which is a compute efficient way to ensure our model is
                invariant to absolute-time shifts.
            </p>

            <div id="arch-diag-box">
                <img src="assets/architecture.svg" alt="architecture-diagram">
                <div class="arch-diag-text" id="left-1">
                    <h2>1. Querying the Spike Train</h2>
                    <p>We compress the variable-length sequence of spike-tokens
                        into 256 latent tokens using a Cross-Attention transformer,
                        which is queried by learned <i>Latent Embeddings</i>.
                    </p>
                    <div class="arch-left-bracket">
                        <img src="assets/left-bracket.svg" style="width:30px;height:250px">
                    </div>
                </div>
                <div class="arch-diag-text" id="left-2">
                    <h2>2. Attention in the Latent Space</h2>
                    <p>The compressed latent tokens are processed by multiple
                        Self-Attention transformer blocks. Since we're only working with 256
                        latents, this is very quick.
                    </p>
                    <div class="arch-left-bracket">
                        <img src="assets/left-bracket.svg" style="width:30px;height:150px">
                    </div>
                </div>
                <div class="arch-diag-text" id="left-3">
                    <h2>3. Querying the Latent Space</h2>
                    <p>A sequence of output-query tokens, one for each behavior
                        sample available in the trial, query the processed latents
                        using a Cross-Attention transformer.
                    </p>
                    <div class="arch-left-bracket">
                        <img src="assets/left-bracket.svg" style="width:30px;height:270px">
                    </div>
                </div>
                <div class="arch-diag-text" id="right-1">
                    <h2>Latent Embeddings</h2>
                    <p>Eight copies of 32 distinct learned embeddings. Each set
                        of 32 is assigned a single timestamp, and these eight sets
                        collectively span a duration of one-second, which is our
                        selected maximum trial length.
                    </p>
                    <div class="arch-right-bracket">
                        <img src="assets/right-bracket.svg" style="width:30px;height:118px">
                    </div>
                </div>
                <div class="arch-diag-text" id="right-2">
                    <h2>Spike Train Input</h2>
                    <p>See <a href="#tokenization">Spike Tokenization</a>
                    </p>
                    <div class="arch-right-bracket">
                        <img src="assets/right-bracket.svg" style="width:30px;height:118px">
                    </div>
                </div>
                <div class="arch-diag-text" id="right-3">
                    <h2>Behavior Queries</h2>
                    <p>Learned session embeddings allow our model to train on
                        data from multiple sessions, days, and labs, simultaneously.
                        <br />
                        <br />
                        The lack of any structural restrictions on query timestamps
                        allows us to train with datasets using different sampling
                        rates.
                    </p>
                    <div class="arch-right-bracket">
                        <img src="assets/right-bracket.svg" style="width:30px;height:130px">
                    </div>
                </div>
            </div>

        </section>

        </section>

        <section class="section" id="pretraining">
            <h3 class="section-title">
                <a href=""> </a>Training Results
            </h3>
            <p>
            In addition to showing that our proposed model outperforms other approaches on hand velocity decoding when trained on single-session datasets,
             we show that training a single model on all datasets jointly leads to improved performance across the board. 
            </p>

            Sample from
            <select id="data-source">
                <option value="./assets/sample_1.json">Monkey C - 2015/03/11 - Center-Out Task</option>
                <option value="./assets/sample_2.json">Monkey C - 2013/10/28 - Random Target Task</option>
                <!-- Add more options as needed -->
            </select>

            <!-- <canvas class="webgl-decode-sample"></canvas>
            <button id="playButton">Play</button>
        
            <script type="module" src="./decode_sample.js"></script> -->

            <div id="vis-container">
                <div id="vis-spike-plot"></div>
                <!-- <div id="vis-model"></div> -->
                <div id="vis-vel-plots">
                    <div id="vis-vx-plot"></div>
                    <div id="vis-vy-plot"></div>
                </div>
            </div>

            <p>
                As shown in Figure 1, we find that the performance of the model increases with model size, and 
                with the amount of data, and this despite the heterogeneity of the datasets.
            </p>
            <div class="figure">
                <img src="assets/scaling.png" alt="Scaling curves">
                <div class="caption">
                    <p>Figure 1: Scaling curves </p>
                </div>
            </div>


        </section>


        <section class="section" id="finetuning">
            <h3 class="section-title">
                <a href=""> </a>Finetuning Results
            </h3>

            <h3 class="subsection-title">Method</h3>
            <p>We can leverage our pretrained large model, POYO-1, for transfer on new datasets. We can use two different approaches:</p>

            <div id="finetune-methods-container">
                <p><b>1. Unit Identification</b>: In order to transfer to a new neural
                    population with previously unseen neurons, we need a way to first assign them
                    a Unit Embedding. We leverage gradient descent to learn the
                    embeddings of new units, while keeping the rest of the network
                    weights fixed. Notably, the function that maps the neural
                    population activity to behavior is unchanged and is simply
                    transferred to the new dataset. In our experiments, we find that
                    this approach is <a href="#finetune-results">surprisingly effective</a>
                    and allows us to rapidly integrate new datasets into the
                    same underlying model. </p>

                <p><b>2. Full Finetuning</b>: Start with POYO-1, and finetune all the weights.</p>
            </div>
            <br />


            <h3 class="subsection-title">
                Unit Identification in Action
            </h3>
            Choose session:
            <select id="finetune-vis-data-selector">
                <option value="T_RT_08202013.mat">Monkey T -
                    2013/08/20 - Random Target Task</option>
                <option value="C_CO_20161013.mat">Monkey C -
                    2016/10/13 - Center Out Task</option>
            </select>
            <div id="finetune-vis-container">
                <div id="finetune-vis-diag-container">
                    <div id="finetune-vis-emb-container">
                        <div id="finetune-vis-emb"></div>
                        <div id="finetune-vis-session-emb"></div>
                    </div>
                    <img id="finetune-vis-img" src="assets/unit-id.svg" alt="architecture-diagram">
                    <div id="finetune-vis-vel-plots">
                        <div id="finetune-vis-vx-plot"></div>
                        <div id="finetune-vis-vy-plot"></div>
                    </div>
                </div>
                <div id="finetune-vis-controls-container">
                    <button id="finetune-vis-button">
                        <i class="fa fa-play"></i>
                    </button>
                    <input type="range" id="finetune-vis-slider" min="0" max="100" value="0" step="1">
                    <div id="finetune-vis-metrics-container">
                        <div class="finetune-vis-metric"><b>Finetuning Step:</b>
                            <p id="finetune-vis-epoch"></p>
                        </div>
                        <div class="finetune-vis-metric"><b>Test Accuracy (R<sup>2</sup>):</b>
                            <p id="finetune-vis-r2"></p>
                        </div>
                    </div>
                </div>
            </div>


            <h3 class="subsection-title" style="margin-bottom: -10px;" id="finetune-results">
                <a href=""> </a>Results
            </h3>
            <div class="table">
                <table border="1">
                    <thead>
                        <th>Method</th>
                        <th>Monkey C - CO (2)</th>
                        <th>Monkey T - CO (6)</th>
                        <th>Monkey T - RT (6)</th>
                        <th>NLB-Maze (1)</th>
                        <th>NLB-RTT (1)</th>
                    </thead>
                    <tr>
                        <td>Wiener Filter</td>
                        <td>0.8860 ¬± 0.0149</td>
                        <td>0.6387 ¬± 0.0283</td>
                        <td>0.5922 ¬± 0.0901</td>
                        <td>0.7485</td>
                        <td>0.5438</td>
                    </tr>
                    <tr>
                        <td>GRU</td>
                        <td>0.9308 ¬± 0.0257</td>
                        <td>0.8041 ¬± 0.0232</td>
                        <td>0.6967 ¬± 0.1011</td>
                        <td>0.8887</td>
                        <td>0.5951</td>
                    </tr>
                    <tr>
                        <td>MLP</td>
                        <td>0.9498 ¬± 0.0119</td>
                        <td>0.8577 ¬± 0.0242</td>
                        <td>0.7467 ¬± 0.0771</td>
                        <td>0.8794</td>
                        <td>0.6953</td>
                    </tr>
                    <!-- <tr>
                        <td>AutoLFADS + Linear</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>0.9062</td>
                        <td>0.5931</td>
                    </tr> -->
                    <tr>
                        <td>POYO-[Single-session] (from scratch)</td>
                        <td>0.9682 ¬± 0.0111</td>
                        <td>0.9194 ¬± 0.0185</td>
                        <td>0.7800 ¬± 0.0702</td>
                        <td>0.9470</td>
                        <td>0.6850</td>
                    </tr>
                    <tr>
                        <td>POYO-MP + Unit identification</td>
                        <td>0.9675 ¬± 0.0079</td>
                        <td>0.9012 ¬± 0.0271</td>
                        <td>0.7759 ¬± 0.0471</td>
                        <td>0.8962</td>
                        <td>0.7107</td>
                    </tr>
                    <tr>
                        <td>POYO-MP + Full finetune</td>
                        <td>0.9708 ¬± 0.0116</td>
                        <td>0.9379 ¬± 0.0193</td>
                        <td>0.8105 ¬± 0.0561</td>
                        <td>0.9466</td>
                        <td>0.7318</td>
                    </tr>
                    <tr>
                        <td>POYO-1 + Unit identification</td>
                        <td>0.9677 ¬± 0.0096</td>
                        <td>0.9028 ¬± 0.0248</td>
                        <td>0.7788 ¬± 0.0548</td>
                        <td>0.9329</td>
                        <td>0.7294</td>
                    </tr>
                    <tr>
                        <td>POYO-1 + Full finetune</td>
                        <td>0.9683 ¬± 0.0118</td>
                        <td>0.9364 ¬± 0.0132</td>
                        <td>0.8145 ¬± 0.0496</td>
                        <td>0.9482</td>
                        <td>0.7378</td>
                    </tr>
                </table>
                <div class="caption">
                    <p> <b>Table 2:</b> <i>Behavioral decoding results
                            for 16 sessions of neural recordings from four nonhuman
                            primates.</i> All the baselines and the single-session model
                        are trained from scratch, while POYO-MP and POYO-1 are
                        pretrained. The standard deviation is reported over the
                        sessions.
                    </p>
                </div>
            </div>

        </section>

        <section class="section">
            <h3 class="section-title">
                Cite our work
            </h3>
            If you find this useful for your research, please consider citing our work:

            <div class="citation">
                <pre><code></code></pre>
            </div>
        </section>
    </div>

    <!-- <footer class="footer">
    <div class="section content">

        <div style="font-size: 1em;">NerDS Lab</div>
    </div>
</footer> -->

    </div>



</body>

</html>