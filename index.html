<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="keywords" content="">
    <meta name="author" content="Mehdi Azabou, Vinam Arora">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>POYO-1</title>
    <link rel="shortcut icon" href="assets/🧠.ico" />
    <script src="https://kit.fontawesome.com/3875b07657.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="https://bulma.io/css/bulma-docs.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="finetune-vis.css">
    <link rel="stylesheet" href="sample-vis.css">
    <link rel="stylesheet" href="query-vis.css">
    <link rel="stylesheet" href="spike-vis.css">


    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-3.2.2.min.js"></script>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.2.min.js"></script>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.2.min.js"></script>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.2.min.js"></script>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.2.2.min.js"></script>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-api-3.2.2.min.js"></script>
    <script src="./mat4js.read.min.js"></script>

    <!-- <script type="module" src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r153/three.module.js"></script> -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-SMP0QT1S6Z"></script> -->
    <!-- <script>
      window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-SMP0QT1S6Z', {
            'page_path': 'https://multiscale-behavior.github.io/'
        });
    </script> -->
    <script>
        function resizeIframe(obj) {
            obj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';
        }
    </script>
</head>

<body>
    <!-- <script src="./two.min.js"></script> -->
    <section class="section title-section">
        <div>
            <h1 class="page-title">
                POYO-1
            </h1>
            <h2 class="title page-title">
                A Unified, Scalable Framework for Neural Population Decoding
            </h2>
        </div>
        <div class="header authors">
            <a href="https://www.mehai.dev">Mehdi Azabou</a><sup>1</sup>
            <a href="https://www.linkedin.com/in/vinam-arora/">Vinam Arora</a><sup>1</sup>
            <a href="">Venkataramana Ganesh</a><sup>1</sup>
            <a href="">Ximeng Mao</a><sup>2</sup>
            <a href="">Santosh Nachimuthu</a><sup>1</sup>
            <a href="">Michael Mendelson</a><sup>1</sup>
            <a href="">Blake Richards</a><sup>2</sup>
            <a href="">Matthew Perich</a><sup>2</sup>
            <a href="">Guillaume Lajoie</a><sup>2</sup>
            <a href="https://dyerlab.gatech.edu/">Eva L. Dyer</a><sup>1,3</sup>
        </div>
        <div class="affiliations">
            <sup>1</sup> Georgia Institute of Technology, <sup>2</sup> MILA, <sup>3</sup> Emory University
        </div>
        <div class="links">
            <a href="">
                <i class="fa fa-file"></i> Paper
            </a>
            <a href="https://github.com/nerdslab/poyo">
                <i class="fa fa-github"></i>
                Code
            </a>
        </div>


    </section>

    <aside class="sidebar">
        <ul>
            <li> Contents </li>
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#datasets">Datasets</a></li>
            <li><a href="#challenges">Challenges</a></li>
            <!-- <li><a href="#heterogeneity">Data Heterogeneity</a></li> -->
            <!-- <li><a href="#tokenization">Tokenization</a></li> -->
            <li><a href="#method">Method</a></li>
            <li><a href="#pretraining">Pretraining</a></li>
            <li><a href="#finetuning">Finetuning</a></li>
            <!-- Add more sections as needed -->
        </ul>
    </aside>


    <div class="main-content">

        <section class="section" id="abstract">
            <h3 class="section-title">
                Abstract
            </h3>
            <p>
                Our ability to use deep learning approaches to decipher neural activity would likely benefit from
                greater scale, in terms of both the model size and the datasets. However, the integration of many neural
                recordings into one unified model is challenging, as each recording contains the activity of different
                neurons from different individual animals.
            </p>

            <p>
                In this paper, we introduce a training framework and
                architecture designed to model the population dynamics of neural activity across diverse, large-scale
                neural recordings. Our method first tokenizes individual spikes within the dataset to build an efficient
                representation of neural events that captures the fine temporal structure of neural activity. We then
                employ cross-attention and a PerceiverIO backbone to further construct a latent tokenization of neural
                population activities. Utilizing this architecture and training framework, we construct a large- scale
                multi-session model trained on large datasets from seven nonhuman primates, spanning over 158 different
                sessions of recording from over 27,373 neural units and over 100 hours of recordings.
            </p>

            <p>
                In a number of
                different tasks, we demonstrate that our pretrained model can be rapidly adapted to new, unseen sessions
                with unspecified neuron correspondence, enabling few-shot performance with minimal labels. This work
                presents a powerful new approach for building deep learning tools to analyze neural data and stakes out
                a clear path to training at scale for neural decoding models.
            </p>

            <!-- <p class="question">
                &#8594 Can we [ ] ?
            </p>

            <p>
                We introduce POYO-1 [...].
            </p> -->
            <!-- 
            <div class="table-of-contents">
                <h2>Table of Contents</h2>
                <ul>
                    <li><a href="#section1"> Section 1</a>: Description.</li>
                    <li><a href="#section2"> Section 2</a>: Description.</li>
                    <li><a href="#section3">Section 3</a>: Description.</li>
                </ul>
            </div> -->
        </section>

        <section class="section" id="datasets">
            <h3 class="section-title">
                <a href="datasets"> </a>Datasets
            </h3>
            <!-- <h3 class="subsection-title">
                <i class="fa-regular fa-circle fa-2xs"></i> Spiking neural activity
            </h3> -->
            <!-- <p>
                One of the key advantages of our approach is its ability to scale to handle large amounts of neural
                data, including sessions from different numbers of neurons, across different tasks and recording setups,
                and from different animals. Thus we set out to build a diverse dataset large enough to test our
                approach.
            </p> -->
            <p>
                We curated a multi-lab dataset with electrophysiological recordings from motor cortical
                regions. In total, we aggregated 178 sessions worth of data,
                with 29,453 units from the primary motor (M1), premotor (PMd), and primary somatosensory (S1)
                regions in the cortex of 9 nonhuman primates.
            </p>

            <div class="table">
                <table border="1">
                    <thead>
                        <tr>
                            <th>Study</th>
                            <th>Regions</th>
                            <th>Tasks</th>
                            <th># Individuals</th>
                            <th># Sessions</th>
                            <th># Units</th>
                            <th># Spikes</th>
                            <th># Behavior Timepoints </th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Perich et al.</td>
                            <td>M1, PMd</td>
                            <td>Center-Out, Random Target</td>
                            <td>4</td>
                            <td>117</td>
                            <td>11,557</td>
                            <td>143M</td>
                            <td>20M</td>
                        </tr>
                        <tr>
                            <td>Churchland et al.</td>
                            <td>M1</td>
                            <td>Center-Out</td>
                            <td>2</td>
                            <td>9</td>
                            <td>1,728</td>
                            <td>706M</td>
                            <td>87M</td>
                        </tr>
                        <tr>
                            <td>Makin et al.</td>
                            <td>M1, S1</td>
                            <td>Random Target</td>
                            <td>2</td>
                            <td>47</td>
                            <td>14,899</td>
                            <td>123M</td>
                            <td>15M</td>
                        </tr>
                        <tr>
                            <td>Flint et al.</td>
                            <td>M1</td>
                            <td>Center-Out</td>
                            <td>1</td>
                            <td>5</td>
                            <td>957</td>
                            <td>7.9M</td>
                            <td>0.3M</td>
                        </tr>
                        <tr>
                            <td>NLB-Maze</td>
                            <td>M1</td>
                            <td>Maze</td>
                            <td>1</td>
                            <td>1</td>
                            <td>182</td>
                            <td>3.6M</td>
                            <td>6.8M</td>
                        </tr>
                        <tr>
                            <td>NLB-RTT</td>
                            <td>M1, S1</td>
                            <td>Random Target</td>
                            <td>1</td>
                            <td>1</td>
                            <td>130</td>
                            <td>1.5M</td>
                            <td>2.8M</td>
                        </tr>
                    </tbody>
                </table>
                <div class="caption">
                    <p> <b>Table 1:</b> Datasets used in this work </p>
                </div>
            </div>

            <!-- <p>We place this in the context of standard
                analyses within a single lab or paper which typically involve 10’s of sessions and a few hundred
                neurons.
            </p> -->

            <p>
                All of these neural recordings were collected while the animals performed various motor tasks that vary
                in their inherent complexity:
            </p>

            <div class="grid-container">
                <!-- Row 1 -->
                <div class="grid-image">
                    <img src="./assets/tasks-04.png" alt="Description for Image 1">
                </div>
                <div class="legend">
                    <b>Center-Out task (CO)</b>: This task is relatively stereotyped, with
                    the animal making a reach to one of eight fixed targets after receiving a go cue, and then
                    returning to the
                    center.
                </div>

                <!-- Row 2 -->
                <div class="grid-image">
                    <img src="./assets/tasks-05.png" alt="Description for Image 2">
                </div>
                <div class="legend">
                    <b>Random Target task (RT)</b>: The animal makes continuous and
                    self-paced
                    movements with new targets appearing in succession at random locations on the screen.
                </div>

                <!-- Row 3 -->
                <div class="grid-image">
                    <img src="./assets/tasks-06.png" alt="Description for Image 3">
                </div>
                <div class="legend">
                    <b>Touchscreen Random Target task (RT)</b>: Instead of using a
                    manipulandum like in the previous experiments, these new datasets use a touch screen.
                </div>

                <!-- Row 4 -->
                <div class="grid-image">
                    <img src="./assets/tasks-07.png" alt="Description for Image 4">
                </div>
                <div class="legend-tagged">
                    <div class="red-tag">
                        Held-out for Testing
                    </div>
                    <p>
                        <b>Maze task</b>: In this task,
                        a monkey performing reaches with instructed delays to visually presented targets while avoiding
                        the
                        boundaries of a virtual maze.
                    </p>
                </div>
            </div>


        </section>

        <section class="section" id="challenges">
            <h3 class="section-title">
                <a href="challenges"> </a>Challenges
            </h3>

            <!-- <p>
                Neurons communicate asynchronously using electrical impulses called spikes. The timing and
                frequency of spikes encode signals that convey information about the external world and coordinate
                the internal dialogue within the brain.
            </p> -->

            <!-- <h3 class="subsection-title">
                <i class="fa-regular fa-circle fa-2xs"></i> Action Potentials
            </h3>
            <p>
                Neural spikes, also known as action potentials, are brief electrical signals that travel along the axon
                of a neuron. They are the primary means by which information is transmitted within the nervous system.
                When a neuron receives sufficient input from other neurons, it generates a spike, which can then
                influence downstream neurons.
            </p>

            <div class="figure">
                <img src="assets/neuron_firing.gif" alt="spiketrain" width="500" height="300">
                <div class="caption">
                    <p> Spike train representation of a neuron's output signal. (Placeholder) </p>
                </div>
            </div> -->

            <!-- <h3 class="subsection-title">
                EPhys Recording
            </h3> -->

            <!-- <p> A device is implented in the brain, and will typically have multiple electrodes.
               An electrode doesn't just pick up the activity of a single neuron. Instead, it detects the electrical
                activity from
                multiple neurons that are in proximity to the electrode tip.
                each neuron has its own unique spiking
                pattern and amplitude based on its type, location, and distance from the electrode. When multiple
                neurons fire action potentials, the electrode picks up a combination of these spikes, which can often
                overlap in time.

                This situation is analogous to the "cocktail party problem" in auditory processing. The cocktail party
                problem refers to the challenge of focusing on a single conversation in a noisy environment, such as a
                party, where multiple people are talking at the same time. Similarly, in neural recordings, the
                challenge is to distinguish and isolate the spiking activity of an individual neuron from the "noise" of
                other nearby neurons.

                To address this, researchers use spike sorting techniques. Spike sorting involves analyzing the recorded
                signals to identify and group spikes that likely originated from the same neuron. This is done based on
                features such as spike shape, amplitude, and timing. 
            </p> -->

            <!-- 
            <h3 class="subsection-title">
                <i class="fa-regular fa-circle fa-2xs"></i> Spike Sorting vs. Threshold crossing
            </h3>

            <p>
                This produces a spike train, which is a sequence of discrete events that represent the times at which
                each neuron fired.
            </p>

            <div class="blocks-container">
                <div class="block-header">
                    <p>Spike-sorting </p>
                </div>
                <div class="block-header">
                    <p>Threshold Crossings </p>
                </div>
                <div class="block">
                    <i class="fa-regular fa-thumbs-up" style="color: #02d453;"></i>
                    <p>Isolates individual neurons, biophyisically realistic units </p>
                </div>

                <div class="block">
                    <i class="fa-regular fa-thumbs-up" style="color: #02d453;"></i>
                    <p>Easy, real-time processing</p>
                </div>
                <div class="block">
                    <i class="fa-regular fa-thumbs-down" style="color: #ff0505;"></i>
                    <p>Complex algorithms for spike sorting that only work offline</p>
                </div>
                <div class="block">
                    <i class="fa-regular fa-thumbs-down" style="color: #ff0505;"></i>
                    <p>Noisy and multi-units</p>
                </div>
            </div> -->

            <h3 class="subsection-title">
                Variability of neural recordings across days and individuals
            </h3>
            <p>
                Outside of knowing which region of the brain was implented, the identity of the neurons that are
                recorded
                is unknown: we are dealing with unlabelled channels. This means that we don't know which neurons are
                being recorded from, or how many we are able to record.
            </p>
            <p>
                Even for the same individual, we don't know whether we're recording from the same neurons across
                multiple
                days. An electrode that was capturing the activity of a particular neuron one day might be slightly
                off-target the next day. This is due to factors like electrode drift.
                <!-- <li> Physical Movement: The brain is a soft tissue, and even minor movements (e.g., due to breathing,
                minor head adjustments, or tissue swelling) can shift the relative position of neurons with respect to
                the electrode. This means that an electrode that was capturing the activity of a particular neuron one
                day might be slightly off-target the next day.</li>

                <li> Electrode Drift: Over time, the electrode itself can shift within the tissue. This can be due to
                various reasons, including the natural healing processes of the tissue around the implanted electrode,
                or mechanical factors related to the electrode's design.</li>

                <li> Tissue Response: The insertion and presence of an electrode can cause a tissue response. This
                might include inflammation, gliosis (formation of a scar-like tissue around the electrode), or neuronal
                death around the insertion site. Over time, these changes can alter the local neural environment,
                affecting which neurons are in proximity to the electrode.</li>

                <li> Neuronal Variability: Even if you were recording from the same neuron, its activity might
                naturally vary from day to day due to changes in the animal's state, experiences, or other factors. This
                variability can sometimes make it seem as though you're recording from different neurons, even if you
                aren't.</li>

                <li> Signal Quality: The quality of recordings can change over time. Factors like electrode
                degradation, changes in impedance, or accumulation of biological material on the electrode surface can
                alter the signal-to-noise ratio, potentially masking spikes from certain neurons.</li>

                Because of these challenges, while it's possible to record from the same general region of the brain
                across multiple days, it's difficult to guarantee that you're recording from the exact same neurons.
                Researchers often use statistical methods and other techniques to assess the stability of their
                recordings and to determine whether they are likely recording from the same or different neuronal
                populations across sessions.</li> -->
            </p>
            <!-- 
            <div class="blocks-container">
                <div class="block">
                    <i class="fa-solid fa-triangle-exclamation" style="color: #f6de2f;"></i>
                    <p><b>Physical Movement</b>: The brain is a soft tissue that moves e.g., due to breathing,
                        minor head adjustments, or tissue swelling. </p>
                </div>

                <div class="block">
                    <i class="fa-solid fa-triangle-exclamation" style="color: #f6de2f;"></i>
                    <p><b>Electrode Drift</b>: Over time, the electrode itself can shift within the tissue, typically
                        due to the natural healing of the tissue around the implanted electrode.</p>
                </div>
                <div class="block">
                    <i class="fa-solid fa-triangle-exclamation" style="color: #f6de2f;"></i>
                    <p><b>Tissue Response</b>: The insertion and presence of an electrode can cause a tissue response.
                        This might include inflammation, gliosis, or neuronal death around the insertion site.</p>
                </div>
                <div class="block">
                    <i class="fa-solid fa-triangle-exclamation" style="color: #f6de2f;"></i>
                    <p><b>Neuronal Variability</b>: Even if you were recording from the same neuron, its activity might
                        naturally vary from day to day due to changes in the animal's state, experiences, or other
                        factors.</p>
                </div>
                <div class="block">
                    <i class="fa-solid fa-triangle-exclamation" style="color: #f6de2f;"></i>
                    <p><b>Signal Quality</b>: Electrode degradation, changes in impedance, or accumulation of biological
                        material on the electrode
                        surface can alter the signal-to-noise ratio.</p>
                </div>
            </div> -->

            <!-- <p>
                The spiking activity of a population of neurons is recorded by placing electrodes near the neurons,
                and measuring the voltage changes that occur when a neuron fires. 
            </p> -->
            <!-- 
            <div class="figure">
                <img src="assets/electrode_shift.png" alt="Electrode shift">
                <div class="caption">
                    <p>  Over time, the electrode can shift within the tissue, typically
                        due to the natural healing of the tissue around the implanted electrode 
                        and minor movements (e.g., due to breathing,
                        minor head adjustments).</p>
                </div>
            </div> -->



            <!-- </section>

        <section class="section" id="heterogeneity">
            <h3 class="section-title">
                <a href=""> </a>Heterogeneity across Datasets
            </h3> -->

            <h3 class="subsection-title">
                Heterogeneity of behavior across datasets
            </h3>

            <p>
                <!-- <i class="fa-solid fa-arrow-right"></i>  -->
                <!-- <b> Multiple behavioral tasks</b>:  -->
                The datasets were collected
                from multiple labs, using different
                equipment, and with different
                experimental protocols. All of these neural recordings were collected while the animals performed
                motor tasks that vary in their inherent complexity.
            </p>

            <div class="canvas-wrapper">

                <div class="canvas-container">
                    <label class="switch" data-canvas-id="webgl1">
                        <input type="checkbox" class="toggle-input" checked>
                        <span class="slider round"></span>
                    </label>
                    <span class="switch-text">3D</span>
                    <canvas id="webgl1" class="webgl"></canvas>
                    <div class="canvas-caption"><b>Center-Out task (CO)</b>: This task is relatively stereotyped,
                        consisting of only 8 configurations, with pre-movement periods.</div>
                </div>

                <div class="canvas-container">
                    <label class="switch" data-canvas-id="webgl2">
                        <input type="checkbox" class="toggle-input" checked>
                        <span class="slider round"></span>
                    </label>
                    <span class="switch-text">3D</span>
                    <canvas id="webgl2" class="webgl"></canvas>
                    <div class="canvas-caption"><b>Random Target task (RT)</b>: Manipulandum.</div>
                </div>

                <div class="canvas-container">
                    <label class="switch" data-canvas-id="webgl3">
                        <input type="checkbox" class="toggle-input" checked>
                        <span class="slider round"></span>
                    </label>
                    <span class="switch-text">3D</span>
                    <canvas id="webgl3" class="webgl"></canvas>
                    <div class="canvas-caption"><b>Touchscreen Random Target task (RT)</b>: Instead of using a
                        manipulandum like in the previous experiments, these new datasets use a touch screen.
                    </div>
                </div>

                <div class="canvas-container">
                    <label class="switch" data-canvas-id="webgl4">
                        <input type="checkbox" class="toggle-input" checked>
                        <span class="slider round"></span>
                    </label>
                    <span class="switch-text">3D</span>
                    <canvas id="webgl4" class="webgl"></canvas>
                    <div class="canvas-caption"><b>Maze task</b>: In this task,
                        a monkey performing reaches with instructed delays to visually presented targets while avoiding
                        the
                        boundaries of a virtual maze.
                    </div>
                </div>

            </div>
            <!-- <canvas class="webgl"></canvas> -->
            <script type="module" src="./script.js"></script>

            <!-- <p>
                <i class="fa-solid fa-arrow-right"></i> <b>Variability of controllers</b>: While some animals perform
                the tasks using a manipulandum, others
                use a touch screen.
            </p> -->

            <p>
                <i class="fa-solid fa-arrow-right"></i> <b>Variablity in sampling rate</b>: The sampling rate at which
                these behaviors were recorded varies
                from lab to lab (100Hz to 1kHz).
            </p>
        </section>

        <!-- <section class="section" id="tokenization">
            <h3 class="section-title">
                <a href=""> </a>Spike Tokenization
            </h3>
            <h3 class="subsection-title">
                Previous approaches: Binning
            </h3>

            <p>
                Because of the irregular nature of neural spiking activity, established approaches usually rely on
                temporally
                binned analyses: time is divided into discrete intervals, and the number of spikes for a unit within
                each interval is counted. Binned representations regularize the structure of spiking data and enable
                compatibility with existing machine learning frameworks.
            </p>

            <iframe src="./spike_binning.html" width="900" height="400" scrolling="no"
                style="overflow:hidden;"></iframe>

            <p>
                However, the choice of bin size presents a trade-off: longer windows (> 100ms) simplify the temporal
                dimension but sacrifice crucial spike-timing information, while shorter
                windows (1 ms) maintain individual spikes but result in an extremely sparse data format.
            </p>

            <h3 class="subsection-title">
                Our solution
            </h3>

            <p>
                By avoiding the need to aggregate or bin the neural activity, our approach can be more
                computationally efficient than previous methods while still maintaining high decoding accuracy. Overall,
                our choice of tokenizing spikes directly offers a promising new approach to neural decoding that
                combines high temporal resolution with computational efficiency.
            </p>

            <p>
                The neural tokenizer. Based upon these motivations, we propose a novel approach for tokenizing neural
                population activity where each spike is represented as a token. Each token contains information about
                the timing of a spike and the unique identity of its unit (the neuron it belongs to). As such, we
                represent our input as a set of tokens of arbitrary length. This is different from the established
                approach of temporal binning (counting the number of spikes in fixed temporal windows), which is
                typically used with neural data. Because we represent the input data as a set of arbitrary size we never
                define or fix the expected number of units, so the model can ingest arbitrary populations of units, and
                thus can be trained across many datasets. At the same time, by representing the time of each spike and
                avoiding representing the times where the neuron doesn’t spike, we avoid the vectors containing mostly
                zeros, as is common in binned representations of neural activity.
            </p>

            <p>
                To make this precise, we assign each unit a unique identifier and a corresponding D-dimensional
                learnable embedding. Let Embed(·) denote a lookup table that associates each unit to its unit embedding.
                Akin to word embeddings that encode semantic meaning and relationship between words, the unit embedding
                space encodes something about the “speaker’s” identity and role in neural computation. A unit fires a
                sequence of spikes in a window of time [0, T ]. Each spike i will be represented by a token
                characterized by (xi, ti), where xi = Embed(spike i′s unit) is the token’s embedding and ti is the spike
                timestamp. Collectively, and for an arbitrary set of units, we combine all the spikes into a sequence of
                length M . The population neural activity is represented by [x1 , . . . , xM ] and their corresponding
                times [t1, . . . , tM ]. While T represents the size of our context window, M will vary depending on the
                number of units in the population and their firing rates (see Figure 1). Note that all spikes from a
                specific unit have the same embedding xi, and only differ in their timing.
            </p>

        </section> -->


        <section class="section" id="method">
            <h3 class="section-title">
                <a href=""> </a> Method
            </h3>

            <h3 class="subsection-title">
                Querying the spiking data
            </h3>
            <p>
                The output of the latent encoder is the latent sequence of size N , while the desired output can be any
                arbitrary sequence of length P . Let us consider the task of hand velocity decoding for example, in the
                context window [0, T], the length of the output sequence will depend on the sampling frequency. Since
                we aspire to train on datasets sourced from various labs, the sampling rate can differ significantly. We
                thus need a flexible mechanism for predicting outputs of varying lengths and querying from the neural
                activity at specific points in time.
            </p>

            <div id="spike-vis-container-1">
                <div id="left-container">
                    <div id="spike-vis-1"></div>
                    <img src="assets/tokenization_arrow.svg"
                        style="width: 280px; padding-left: 150px; padding-top: 10px;">
                    <div id="token-vis-1"></div>
                </div>
                <!-- <div id="token-container">
                    
                </div> -->
                <div id="encoder">
                    <img src="assets/encoder.svg" style="height:360px">
                </div>
            </div>
            <script src="spike-vis.js"></script>


            <h3 class="subsection-title">
                Self-Attention in the latent space
            </h3>
            <p>
                The output of the latent encoder is the latent sequence of size N , while the desired output can be any
                arbitrary sequence of length P . Let us consider the task of hand velocity decoding for example, in the
                context window [0, T], the length of the output sequence will depend on the sampling frequency. Since
                we aspire to train on datasets sourced from various labs, the sampling rate can differ significantly. We
                thus need a flexible mechanism for predicting outputs of varying lengths and querying from the neural
                activity at specific points in time.
            </p>

            <div style="margin: auto; text-align: center; padding-top: 1em; padding-bottom: 1em;">
                <img src="assets/selfattn.svg" style="height:125px;">
            </div>

            <h3 class="subsection-title">
                Querying the latent space
            </h3>
            <p>
                The output of the latent encoder is the latent sequence of size N , while the desired output can be any
                arbitrary sequence of length P . Let us consider the task of hand velocity decoding for example, in the
                context window [0, T], the length of the output sequence will depend on the sampling frequency. Since
                we aspire to train on datasets sourced from various labs, the sampling rate can differ significantly. We
                thus need a flexible mechanism for predicting outputs of varying lengths and querying from the neural
                activity at specific points in time.
            </p>

            <div id="query-vis-container-1">
                <div class="square" id="animatedSquare"></div>
                <div id="left-container">
                    <div id="query-vis-1"></div>
                    <div id="decoder">
                        <img src="assets/Decoder.svg" style="height:250px">
                    </div>
                </div>
                <div id="output-container">
                    <div id="output-vis-1"></div>
                    <div id="output-vis-2"></div>
                </div>
            </div>
            <script src="query-vis.js"></script>


            <p>
                With our tokenization techniques, the problem of Neural Decoding
                boils down to learning a model that converts a sequence of spike
                tokens to a sequence of hand-velocities.
            </p>
            <p>
                Our approach utilizes the Transformer architecture, recognized for
                its excellence in handling sequence-to-sequence transformations.
                Given the potentially large number of spikes in a single
                experimental trial, we've adopted DeepMind's PerceiverIO
                architecture <a href="https://www.deepmind.com/open-source/perceiver-io">
                    [Jeagle et al.]</a> instead of the standard transformer
                <a href="https://arxiv.org/abs/1706.03762">[Vaswani et al.]</a>.
                To inject token timestamps into the attention
                blocks, we use Rotary Position Embeddings
                <a href="https://arxiv.org/abs/2104.09864">[Su et. al.]</a>,
                which is a compute efficient way to ensure our model is
                invariant to absolute-time shifts.
            </p>

            <div id="arch-diag-box">
                <img src="assets/architecture.svg" alt="architecture-diagram">
                <div class="arch-diag-text" id="left-1">
                    <h2>1. Querying the Spike Train</h2>
                    <p>We compress the variable-length sequence of spike-tokens
                        into 256 latent tokens using a Cross-Attention transformer,
                        which is queried by learned <i>Latent Embeddings</i>.
                    </p>
                    <div class="arch-left-bracket">
                        <img src="assets/left-bracket.svg" style="width:30px;height:250px">
                    </div>
                </div>
                <div class="arch-diag-text" id="left-2">
                    <h2>2. Attention in the Latent Space</h2>
                    <p>The compressed latent tokens are processed by multiple
                        Self-Attention transformer blocks. Since we're only working with 256
                        latents, this is very quick.
                    </p>
                    <div class="arch-left-bracket">
                        <img src="assets/left-bracket.svg" style="width:30px;height:150px">
                    </div>
                </div>
                <div class="arch-diag-text" id="left-3">
                    <h2>3. Querying the Latent Space</h2>
                    <p>A sequence of output-query tokens, one for each behavior
                        sample available in the trial, query the processed latents
                        using a Cross-Attention transformer.
                    </p>
                    <div class="arch-left-bracket">
                        <img src="assets/left-bracket.svg" style="width:30px;height:270px">
                    </div>
                </div>
                <div class="arch-diag-text" id="right-1">
                    <h2>Latent Embeddings</h2>
                    <p>Eight copies of 32 distinct learned embeddings. Each set
                        of 32 is assigned a single timestamp, and these eight sets
                        collectively span a duration of one-second, which is our
                        selected maximum trial length.
                    </p>
                    <div class="arch-right-bracket">
                        <img src="assets/right-bracket.svg" style="width:30px;height:118px">
                    </div>
                </div>
                <div class="arch-diag-text" id="right-2">
                    <h2>Spike Train Input</h2>
                    <p>See <a href="#tokenization">Spike Tokenization</a>
                    </p>
                    <div class="arch-right-bracket">
                        <img src="assets/right-bracket.svg" style="width:30px;height:118px">
                    </div>
                </div>
                <div class="arch-diag-text" id="right-3">
                    <h2>Behavior Queries</h2>
                    <p>Learned session embeddings allow our model to train on
                        data from multiple sessions, days, and labs, simultaneously.
                        <br />
                        <br />
                        The lack of any structural restrictions on query timestamps
                        allows us to train with datasets using different sampling
                        rates.
                    </p>
                    <div class="arch-right-bracket">
                        <img src="assets/right-bracket.svg" style="width:30px;height:130px">
                    </div>
                </div>
            </div>

        </section>

        </section>

        <section class="section" id="pretraining">
            <h3 class="section-title">
                <a href=""> </a>Pretraining
            </h3>
            Sample from
            <select id="data-source">
                <option value="./assets/sample_1.json">Monkey C - 2015/03/11 - Center-Out Task</option>
                <option value="./assets/sample_2.json">Monkey C - 2013/10/28 - Random Target Task</option>
                <!-- Add more options as needed -->
            </select>

            <!-- <canvas class="webgl-decode-sample"></canvas>
            <button id="playButton">Play</button>
        
            <script type="module" src="./decode_sample.js"></script> -->

            <div id="vis-container">
                <div id="vis-spike-plot"></div>
                <!-- <div id="vis-model"></div> -->
                <div id="vis-vel-plots">
                    <div id="vis-vx-plot"></div>
                    <div id="vis-vy-plot"></div>
                </div>
            </div>
            <script src="sample-vis.js"></script>

        </section>


        <section class="section" id="finetuning">
            <h3 class="section-title">
                <a href=""> </a>Finetuning
            </h3>

            <h3 class="subsection-title">Method</h3>
            <p>There are two ways to transfer POYO-1 on new datasets:</p>

            <div id="finetune-methods-container">
                <p><b>1. Unit Identification</b>: In order to transfer to a new neural
                    population with unseen neurons, we need a way to first assign them
                    a Unit Embedding. We leverage gradient descent to learn the
                    embeddings of new units, while keeping the rest of the network
                    weights fixed. Notably, the function that maps the neural
                    population activity to behavior is unchanged and is simply
                    transferred to the new dataset. In our experiments, we find that
                    this approach is surprisingly effective and allows us to rapidly
                    integrate new datasets into the same underlying model. </p>

                <p><b>2. Full Finetuning</b>: Train all the weights.</p>
            </div>
            <br/>


            Choose session: 
            <select id="finetune-vis-data-selector">
                <option
                value="T_RT_08202013.mat">Monkey T -
                2013/08/20 - Random Target Task</option>
                <option
                value="C_CO_20161013.mat">Monkey C -
                2016/10/13 - Center Out Task</option>
            </select>
            <div id="finetune-vis-container">
                <div id="finetune-vis-diag-container">
                    <div id="finetune-vis-emb-container">
                        <div id="finetune-vis-emb"></div>
                        <div id="finetune-vis-session-emb"></div>
                    </div>
                    <img id="finetune-vis-img" src="assets/unit-id.svg" alt="architecture-diagram">
                    <div id="finetune-vis-vel-plots">
                        <div id="finetune-vis-vx-plot"></div>
                        <div id="finetune-vis-vy-plot"></div>
                    </div>
                </div>
                <div id="finetune-vis-controls-container">
                    <button id="finetune-vis-button">
                        <i class="fa fa-play"></i>
                    </button>
                    <input type="range" id="finetune-vis-slider" min="0" max="100" value="0" step="1">
                    <div id="finetune-vis-metrics-container">
                        <div class="finetune-vis-metric"><b>Finetuning Step:</b>
                            <p id="finetune-vis-epoch"></p>
                        </div>
                        <div class="finetune-vis-metric"><b>Test Accuracy (R<sup>2</sup>):</b>
                            <p id="finetune-vis-r2"></p>
                        </div>
                    </div>
                </div>
            </div>
            <script src="finetune-vis.js"></script>


            <h3 class="subsection-title" style="margin-bottom: -10px;">Results</h3>
            <div class="table">
                <table border="1">
                    <thead>
                        <th>Method</th>
                        <th>Monkey C - CO (2)</th>
                        <th>Monkey T - CO (6)</th>
                        <th>Monkey T - RT (6)</th>
                        <th>NLB-Maze (1)</th>
                        <th>NLB-RTT (1)</th>
                    </thead>
                    <tr>
                        <td>Wiener Filter</td>
                        <td>0.8860 ± 0.0149</td>
                        <td>0.6387 ± 0.0283</td>
                        <td>0.5922 ± 0.0901</td>
                        <td>0.7485</td>
                        <td>0.5438</td>
                    </tr>
                    <tr>
                        <td>GRU</td>
                        <td>0.9308 ± 0.0257</td>
                        <td>0.8041 ± 0.0232</td>
                        <td>0.6967 ± 0.1011</td>
                        <td>0.8887</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>MLP</td>
                        <td>0.9498 ± 0.0119</td>
                        <td>0.8577 ± 0.0242</td>
                        <td>0.7467 ± 0.0771</td>
                        <td>0.8794</td>
                        <td>0.6953</td>
                    </tr>
                    <tr>
                        <td>AutoLFADS + Linear</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>0.9062</td>
                        <td>0.5931</td>
                    </tr>
                    <tr>
                        <td>POYO-[Single-session]</td>
                        <td>0.9682 ± 0.0111</td>
                        <td>0.9194 ± 0.0185</td>
                        <td>0.7800 ± 0.0702</td>
                        <td>0.9470</td>
                        <td>0.6850</td>
                    </tr>
                    <tr>
                        <td>POYO-MP + Unit id</td>
                        <td>0.9675 ± 0.0079</td>
                        <td>0.9012 ± 0.0271</td>
                        <td>0.7759 ± 0.0471</td>
                        <td>0.8962</td>
                        <td>0.7107</td>
                    </tr>
                    <tr>
                        <td>POYO-MP + Finetune</td>
                        <td>0.9708 ± 0.0116</td>
                        <td>0.9379 ± 0.0193</td>
                        <td>0.8105 ± 0.0561</td>
                        <td>0.9466</td>
                        <td>0.7318</td>
                    </tr>
                    <tr>
                        <td>POYO-1 + Unit id</td>
                        <td>0.9677 ± 0.0096</td>
                        <td>0.9028 ± 0.0248</td>
                        <td>0.7788 ± 0.0548</td>
                        <td>0.9329</td>
                        <td>0.7294</td>
                    </tr>
                    <tr>
                        <td>POYO-1 + Finetune</td>
                        <td>0.9683 ± 0.0118</td>
                        <td>0.9364 ± 0.0132</td>
                        <td>0.8145 ± 0.0496</td>
                        <td>0.9482</td>
                        <td>0.7378</td>
                    </tr>
                </table>
            </div>

        </section>

        <section class="section">
            <h3 class="section-title">
                Cite our work
            </h3>
            If you find this useful for your research, please consider citing our work:

            <div class="citation">
                <pre><code></code></pre>
            </div>
        </section>
    </div>

    <!-- <footer class="footer">
    <div class="section content">

        <div style="font-size: 1em;">NerDS Lab</div>
    </div>
</footer> -->

    </div>



</body>

</html>
